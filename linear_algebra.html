<!-- Start of code generated using Learning Module Creator -->

<!-- Module 1 - Introduction to Vectors Begins Here -->
<h2>Week 1 - Introduction to Vectors</h2>

	<h3>Readings</h3>
		<p>Please read sections 1.1, 1.2, and 1.3.</p>

	<h3>Words of Advice</h3>
		<ul>
			<li>The picture for problem 1.1 23 is on the previous page.</li>
			<li>The whole course is about linear combinations and vectors, don't slack off here.</li>
			<li>It might be a legit idea to do all the problems in this seciton.</li>
			<li>The other book chooses to delay discussions of dot products until very late. I personally believe this is a mistake.</li>
			<li>Do the worked out examples and try not to look at the solution key until you've actually worked on a problem for a bit.</li>
			<li>Answers don't count for much, explanations are where all the points are, so get used to writing out your thoughts for each step.</li>
			<li>You should be explaining at a level your fellows would understand.</li>
			<li>1.2 numbers 26 and 27 are meant to be a single problem.</li>
			<li>Problems 4 and 5 in 1.3 are the heart of the subject. Problem 4 is the 'Column multiplication picture' and problem 5 is the 'row multiplication picture'.</li>
			<li>For problem 7 in 1.3, the xs are unknown but fixed values, first assume that they aren't 0. After you're done, consider the possiblity that the xs in  are 0, is the conclusion wrong? </li>
			<li>Hints for problems 4 and 5 from 1.3 can be found by reading 2.1.</li>
		</ul>

	<h3>Work</h3>
		<ul>
			<li>In section 1.1, please do problems 1, 3, 4, 6, 9, 11, 13, 18, 23, 26-29, and 31.</li>
			<li>In section 1.2, please do problems 1, 2, 6, 10, 13, 16, 20, 26/27 (these are a single problem), 29, 30 .</li>
			<li>In section 1.3, please do problems 1, 2-6, 7, 14.</li>
		</ul>

<!-- End of Module 1 -->
<!-- Module 2 - Systems and Elimination Begins Here -->
<h2>Week 2 - Systems and Elimination</h2>

	<h3>Readings</h3>
		<p>Please read sections 2.1, 2.2, and 2.3.</p>

	<h3>Words of Advice</h3>
		<ul>
			<li>The two pictures (column and row) in section 2.1 are the first step towards understanding the picture on the front of the book.</li>
			<li>They're two sides of the same thought, but seem very different.</li>
			<li>If you solve something using one of them consider trying it with the other. It won't always work, but it is good to consider.</li>
			<li>A matrix is a function. An $m\times n$ matrix is a function from the n dimensional real numbers to the m dimensional real numbers.</li>
			<li>Elimination matricies are fairly tough, I'm keeping the assingments in 2.2 and 2.3 a little light.</li>
			<li>For number 20 in 2.2, to see the three planes described at the beginning, make a triangular tube out of a piece of paper.</li>
			<li>For number 32 in 2.2, you should use some ..., don't fill a page with numbers.</li>
			<li>The answer key answer for number 32 part d is describing the row and column spaces for the second matrix they describe in part c, not the first one.</li>
			<li>Problem 28 in section 2.3 is fairly tough to do without a hint, but it's really simple with one. For reference the 'associative law' is on page 61.</li>
		</ul>

	<h3>Work</h3>
		<ul>
			<li>In section 2.1, please do problems 1, 4-13, 15-17, 19, 20, 22, 26, 28, 31.</li>
			<li>In section 2.2, please do problems 3, 4-6, 11, 13, 20, 24, 32.</li>
			<li>In section 2.3, please do problems 1-3, 6, 7, 12, 13, 16, 17, 19, 24, 27, 28.</li>
		</ul>

<!-- End of Module 2 -->
<!-- Module 3 - Matrix Operations and Inverses Begins Here -->
<h2>Week 3 - Matrix Operations and Inverses</h2>

	<h3>Readings</h3>
		<p>Please read sections 2.4 and 2.5.</p>

	<h3>Words of Advice</h3>
		<ul>
			<li>It is incredibly important that you know that matricies don't commute.</li>
			<li>$AB$ is NOT generally $BA$.</li>
			<li>Even if $AB$ is $BA$, this doesn't mean that $A$ or $B$ commute with pretty much anything else.</li>
			<li>Matrix multiplication is function composition, which is why it's not commutative.</li>
			<li>Exponents (repeated application of the matrix) have the usual rules.</li>
			<li>Matricies are 'linear', meaning (for vectors $x$ and $y$, matrix $A$ and scalar $c$) $A(x+y) = Ax+Ay$ and $Acx = cAx$</li>
			<li>The worked example 2.4B illustrates some important concepts.</li>
			<li>Functions are only invertible if they're one to one.</li>
			<li>The good old horizontal line test is checking this. Specifically, we need to check if there are two inputs that give us the same output.</li>
			<li>For a regular function $f(x)$, this looks like a pair of $a$ and $b$ with $a != b$ and $f(a) = f(b)$.</li>
			<li>In the special case of a matrix, this would look like $x$ and $y$ with $x != y$ and $Ax = Ay$. Because we know $A$ is linear, we can rearrange this to $Ax-Ay=0$, which can be further rearranged into $A(x - y) =0$.</li>
			<li>Ultimately, a matrix fails to be invertible if there is a nonzero vector $x$ with $Ax=0$. This will keep coming back up.</li>
			<li>The determinant gets a casual mention in this section, some approaches to this topic focus almost exclusively on the determinant. Those appraches are less effective in my opinion, but they do exist.</li>
			<li>Worked example 2.5B is a pretty crucial concept, expect to use this repeatedly.</li>
			<li>Order Matters. A supervising professor of mine wrote a precalculus book that had this phrase bolded every other paragraph or so. We made fun of it a bit, but it turns out it's just super solid life advice.</li>
		</ul>

	<h3>Work</h3>
		<ul>
			<li>In section 2.4, please do problems 1-3, 5-7, 11, 15, 22, 23, 32, 34.</li>
			<li>In section 2.5, please do problems 1, 2, 7-9, 11, 12, 15, 18, 22, 29, 30, 39.</li>
		</ul>

<!-- End of Module 3 -->
<!-- Module 4 - LU Factorization and Transposes Begins Here -->
<h2>Week 4 - LU Factorization and Transposes</h2>

	<h3>Readings</h3>
		<p>Please read sections 2.5 and 2.6.</p>

	<h3>Words of Advice</h3>
		<ul>
			<li>Factorization seems fairly silly, but as we develop this area, we'll find it increasingly useful.</li>
			<li>Linear algebra calculations are crucial to a large number of computer systems. Keeping those computer systems efficient is a huge task and factorizations often let us cut large amounts of calculations.</li>
			<li>Those of you who took Calc 3 will recall how totally crucial dot products were. It will come as no surprise that dot products are crucial here.</li>
			<li>Transpose matricies let us capture some of that utility for a more general application.</li>
			<li>Transpose matricies will continue to be seen throughout the class. In fact, they're crucial to understanding the cover art.</li>
			<li>The transpose of a matrix $A$ can also be thought of as $A$ flipped across the main diagonal.</li>
			<li>A somewhat useful notational trick is to recognize $(Ax)\cdot y = (Ax)^Ty = x^TA^Ty = x\cdot A^Ty$. </li>
			<li>There's some discussion of this idea on page 121 with respect to derivatives.</li>
		</ul>

	<h3>Work</h3>
		<ul>
			<li>In section 2.6, please do problems 1, 2, 5-8, 12, 13.</li>
			<li>In section 2.7, please do problems 1-4, 7bcd, 9, 10, 13a, 16, 17a, 28, 30, 37, 39.</li>
		</ul>

<!-- End of Module 4 -->
<!-- Module 5 - Midterm 1 and Vector Spaces Begins Here -->
<h2>Week 5 - Midterm 1 and Vector Spaces</h2>

	<h3>Readings</h3>
		<p>Please read sections 3.1.</p>

	<h3>Words of Advice</h3>
		<ul>
			<li>Fully explain all of your answers to the test. There's no credit for solutions without explanation.</li>
			<li>Explanations should cite facts from the book by page number and location. For example, 'top of page 123' or 'eqn (2) on page 123'.</li>
			<li>Justification should be provided for each step.</li>
			<li>You get credit for sound mathematical reasoning and careful explanation.</li>
			<li>Your explanations should be aimed at your peers level, not at me or Prof. Strang.</li>
			<li>Tests are assumed to be open book and notes, but are individual assessments.</li>
			<li>Please don't collaborate on tests.</li>
			<li>After you've finished your test, take on section 3.1.</li>
			<li>Section 3.1 is crucial, be sure to devote adequate time to it.</li>
			<li>A real vector space is a set of vectors and a pair of operations 'vector addition' and 'scalar multiplication' that is closed (see top of p125) under both operations and satisfys items (1) - (8) on p130.</li>
			<li>Vectors in these vector spaces do not need to look like the usual vectors in $\mathbb{R^n}$.</li>
			<li>Vector addition and scalar multiplication are not always definied in the usual way.</li>
			<li>The 'zero vector' need not actually be 0. See problem 3.</li>
			<li>Vector multiplication is rarely defined. (Dot and cross product aren't really multiplication.) Vector division is almost never defined, with the notable exceptions of $\mathbb{R^1}$ and $\mathbb{C}$.</li>
			<li>If the homework in section 3.1 seems easy, you're either doing it wrong or you're ready for higher math.</li>
			<li>The column space of a matrix is a critical concept moving forward, don't neglect it.</li>
			<li>There is at least one column space in the cover art.</li>
			<li>The symbol $\cup$ is called the 'union'. $S\cup T$ is the set of all things that are in $S$ or in $T$. For example $\{1,2,3\} \cup \{2,4,6 \} = \{1,2,3,4,6 \}$.</li>
			<li>The symbol $\cap$ is called the 'intersection'. $S\cap T$ is the set of all things that are in both $S$ and $T$. For example $\{1,2,3\} \cap \{2,4,6 \} = \{2 \}$.</li>
			<li>There will be several important subspaces associated to a matrix, all of which are the span of a set of vectors. The concept of span becomes more important as we proceed.</li>
			<li>'Subspaces are flat.' Consider this statement carefully and see if you can justify it.</li>
		</ul>

	<h3>Work</h3>
		<ul>
			<li>In section 3.1, please do problems 1-11, 14, 15, 17, 19, 22-24, 30.</li>
		</ul>

<!-- End of Module 5 -->
<!-- Module 6 - Nullspaces and Complete Solutions Begins Here -->
<h2>Week 6 - Nullspaces and Complete Solutions</h2>

	<h3>Readings</h3>
		<p>Please read sections 3.2 and 3.3.</p>

	<h3>Words of Advice</h3>
		<ul>
			<li>The rank of a matrix is very important, it's the dimension of the image (also called the range).</li>
			<li>The lines on the top of p140 shouldn't be ignored, they're going to be used alot.</li>
			<li>We kinda lied to you about domains and ranges. Let's clear that up a little bit. The domain of a function is the set of valid inputs, the range is the set of all outputs, and a third set called the 'codomain' is the space containing the range.</li>
			<li>For example, consider the matrix $A = \begin{bmatrix} 1 & 0 & 0\\ 0 & 0 &0 \end{bmatrix}$. This function has a domain of $\mathbb{R^3}$, a range of $\text{span}\begin{bmatrix} 1\\0 \end{bmatrix}$, and a codomain of $\mathbb{R^2}$.</li>
			<li>The Counting Theorem on p140 and in problem 4 is crucial to the sizes of the boxes on the cover art.</li>
			<li>Problems 48-50 are important, but conceptually hard and the solution manual explanations are a bit short in my opinion. Please make your explanations more complete. For problem 49, you may use that $rank(A) = rank(A^T)$ for any matrix $A$.</li>
			<li>It's sometimes said that mathematicians really only use three numbers, 0, 1, and $\infty$. This section illustrates the truth of that.[:)]</li>
			<li>All of this section basically rests on the idea that $Ax= b$ is equivalent to $A(x_p+x_s) = Ax_p +Ax_s = b + 0$.</li>
			<li>Now you can probably explain why the cover art contains $Ay = b$, $y=x+z$, $Ax= b$, and $Az=0$. If you'd like to check your understanding, please explain it on your coversheet for this module.</li>
		</ul>

	<h3>Work</h3>
		<ul>
			<li>In section 3.2, please do problems 1, 2, 4, 5, 8, 9, 12, 16, 20-22, 24a, 29, 33, 39, 41, 48-50..</li>
			<li>In section 3.3, please do problems 3, 4, 6, 10, 11, 12, 13, 22, 24, 31, 34ac.</li>
		</ul>

<!-- End of Module 6 -->
<!-- Module 7 - Dimensions Begins Here -->
<h2>Week 7 - Dimensions</h2>

	<h3>Readings</h3>
		<p>Please read sections 3.4 and 3.5.</p>

	<h3>Words of Advice</h3>
		<ul>
			<li>Other dimensions, not quite what sci-fi makes them out to be eh?</li>
			<li>I'd prefer if the author refered to sets of vectors being linearly independant rather than sequences of vectors because the ordering is not important.</li>
			<li>No set containing the zero vector can be linearly independent.</li>
			<li>The geometry of independence is a good intuition to have.</li>
			<li>A space or subspace will have a great many possible basis sets to choose from.</li>
			<li>Function spaces are super cool.</li>
			<li>The zero vector space (only the zero vector) is a little confusing, because it is dimension 0 and its basis is the empty set (the set of no vectors). See top of p. 172.</li>
			<li>Spans of sets are subspaces and subspaces are flat.</li>
			<li>For problem 3.4 45, you'll probably want to consider a basis for each V and W.</li>
			<li>3.5, the big picture! Here we go, time to think about those rectangles in the cover art!</li>
			<li>Remember problems 48-50 in the nullspace section? The results are summarized again in Wroked Example 3.5B.</li>
			<li>Personally, I tend to say 'the nullspace of A transpose' rather than 'left nullspace of A'. Perhaps this is a personality deficit, but I prefer to think of $A$ as a map from left to right of the Big Picture and $A^T$ as a map from right to left.</li>
			<li>The Fundamental Theorem and the Big Picture in this section are the foundations of linear algebra. Everything before this was roughly so that you could understand this, and everything after it is basically a consequence of it. Frankly, this section is the subject of Linear Algebra.</li>
			<li>All the mysteries of the cover art are not yet solved, but this should clarify a great deal of it.</li>
			<li>There is an element that could be added to the cover art involving the dimensions of the spaces, what is it?</li>
		</ul>

	<h3>Work</h3>
		<ul>
			<li>In section 3.4, please do problems 1, 2, 5, 8, 9, 11, 15, 16, 24, 26, 31, 32, 35, 38, 45.</li>
			<li>In section 3.5, please do problems 1, 2, 4, 5, 6, 10, 11, 13ab, 16, 24, 29.</li>
		</ul>

<!-- End of Module 7 -->
<!-- Module 8 - Orthogonality and Projections Begins Here -->
<h2>Week 8 - Orthogonality and Projections</h2>

	<h3>Readings</h3>
		<p>Please read sections 4.1 and 4.2.</p>

	<h3>Words of Advice</h3>
		<ul>
			<li>Orthogonal complements, super cool and also maybe you realized this a long time ago. Some folks do, usually because they notice that a vector in the nullspace must be perpendicular to the rows and the two spaces dimensions add to the domains.</li>
			<li>More Big Picture! On p 198 you can see a possible subtle change to the cover art. See how the Nullspaces are different sizes, but the row and col spaces are the same size? This is what I was asking for in the last module.</li>
			<li>The decomposition on page 199 ($x = x_r + x_n$) is somewhat surprising and also cool. It's the $y = x+z$ from the cover art.</li>
			<li>Worked Example 4.1A is a good understanding check.</li>
			<li>Problems 6 and 7 in 4.1 are not required, but if you're interested in applied math, the Fredholm Alternative makes several appearances.</li>
			<li>Problem 4.1 9 will come up again in the next couple sections, keep your eye on it.</li>
			<li>Students comfortable with calc 3 may prefer to phrase one dimensional projections in terms of dot products, but rephrasing in terms of transposes will let us lift this idea to matricies in general.</li>
			<li>Projection matricies onto the axises or coordinate planes are a useful tool in a number of contexts. The vast majority of applications of matricies outside of Linear Algebra utilize either projection or rotation matricies.</li>
			<li>To make the projection matrix on page 208 sound as fancy as possible, you can refer to it as the outside-in product of a with itself. This isn't serious, but you might notice that the top is the outer product of a with a and the bottom is the inner product of a with a.</li>
			<li>Be careful, order matters.</li>
			<li>$A^TA$ is invertible if A has linearly indep columns, but in this context A is rectanglular, therefore not invertible.</li>
			<li>$P^2 = P$ is sometimes taken to be the definition of a projection, you might convince yourself that this makes sense. It'll be the focus of some problems.</li>
			<li>Notice that there's a picture for problem 5.</li>
			<li>If you have extra time, question 34 is cool and doesn't have an answer in the solutions. Perhaps you can prove this.</li>
		</ul>

	<h3>Work</h3>
		<ul>
			<li>In section 4.1, please do problems 1-4, 5a, 9-11, 13, 16(See page 195 for (2)), .</li>
			<li>In section 4.2, please do problems 1, 3, 5, 8, 11-13, 17, 18, 21, 22, 23, 24, 25, 32, 34 optional.</li>
		</ul>

<!-- End of Module 8 -->
<!-- Module 9 - Least Squares and Gram-Schmidt Begins Here -->
<h2>Week 9 - Least Squares and Gram-Schmidt</h2>

	<h3>Readings</h3>
		<p>Please read sections 4.3 and 4.4.</p>

	<h3>Words of Advice</h3>
		<ul>
			<li>Least Squares regression is probably the most common statistics tool.</li>
			<li>The exceptionally cool thing about this section is that we learn that least squares lines aren't really any easier (in theory) than any other least squares polynomial fit.</li>
			<li>All of this is a little contrived by hand. Programming a computer to do this quickly is super easy.</li>
			<li>Our usual basis is an orthonormal basis, and the desire to go back to it or think in terms of it should motivate us to find more othornormal basises.</li>
			<li>The rotation matricies mentioned in Example 1 are a very common tool.</li>
			<li>Notice that orthogonal matricies are square.</li>
			<li>Careful, there are permutation matricies in this section, don't get them mixed up with projections.</li>
			<li>Squaring problem 4b and problem 10 in 4.4 seems like a tall order. Why are both possible? Read carefully.</li>
		</ul>

	<h3>Work</h3>
		<ul>
			<li>In section 4.3, please do problems 1-3, 5, 9, 10, 17, 20, 21, 25.</li>
			<li>In section 4.4, please do problems 1, 2, 4bc, 5, 10, 11, 13, 17, 18, 20, 21, 22.</li>
		</ul>

<!-- End of Module 9 -->
<!-- Module 10 - Test 2 and Determinants Begins Here -->
<h2>Week 10 - Test 2 and Determinants</h2>

	<h3>Readings</h3>
		<p>Please read sections 5.1.</p>

	<h3>Words of Advice</h3>
		<ul>
			<li>Much of the advice I gave for the first test applies here, but I'm writing this before I've seen your first exams, so please bear in mind any advice I gave you upon the retrun of those tests.</li>
			<li>Fully explain all of your answers to the test. There's no credit for solutions without explanation.</li>
			<li>Explanations should cite facts from the book by page number and location. For example, 'top of page 123' or 'eqn (2) on page 123'.</li>
			<li>Justification should be provided for each step.</li>
			<li>You get credit for sound mathematical reasoning and careful explanation.</li>
			<li>Your explanations should be aimed at your peers level, not at me or Prof. Strang.</li>
			<li>Tests are assumed to be open book and notes, but are individual assessments.</li>
			<li>Please don't collaborate on tests.</li>
			<li>After you've finished your test, take on section 5.1.</li>
			<li>Section 5.1 is crucial, be sure to devote adequate time to it.</li>
			<li>IMPORTANT: Determinants are only defined for square matricies.</li>
			<li>Many other texts devote a large portion of the start of this course to calculation of the determinant. Personally I find that approach to be boring, because I find calculation by hand to be boring. The determinant is so useful that it can't be ignored, so here we are.</li>
			<li>A computer or a calculator is the right tool for finding a determinant. We'll do some by hand here, but it's not ideal.</li>
			<li>In chapter 6 we'll end up doing more than a little calculation of determinants by hand. In realiztic applications, these are usually calculated using a computer.</li>
			<li>The problems involving $A-\lambda I$ are foreshadowing for chapter 6.</li>
		</ul>

	<h3>Work</h3>
		<ul>
			<li>In section 5.1, please do problems 1-4, 6, 7 (only the first Q), 8,  10, 22, 23, 28.</li>
		</ul>

<!-- End of Module 10 -->
<!-- Module 11 - Determinant Formulas Begins Here -->
<h2>Week 11 - Determinant Formulas</h2>

	<h3>Readings</h3>
		<p>Please read sections 5.2 and 5.3.</p>

	<h3>Words of Advice</h3>
		<ul>
			<li>There are a ton of scary ass formulas in here. The cofactor formula is the one that gets the most use in real life.</li>
			<li>Most of these calculations are done recursively on computers. Recursion is when a computer program starts another copy of itself, usually to do a smaller calculation and report back.</li>
			<li>Notice that the cofactors switch signs.</li>
			<li>Cofactors are especially useful when there are lots of zeros.</li>
			<li>Problem 5.2 15 and 16 are a bit challenging to see because you have to go just a little deeper than you'd expect.</li>
			<li>Cramer's Rule lends itself to computer calculation of matrix inverses because it involves no possibility of required row swapping.</li>
			<li>It's straightforward execution is what makes it valuable, sadly it's not particularly easy to calculate unless you're a computer.</li>
			<li>The area interpretations of the determinant should come as no surprise to those who took calc 3.</li>
			<li>For those not having taken calc 3, the cross product is an important tool there for finding the areas of surfaces.</li>
			<li>Example 6 is another perspective on double integration in polar coordinates. Anybody care to extrapolate to cylindrical or spherical?</li>
			<li>You can skip 27 and 28 if you didn't take calc 3.</li>
		</ul>

	<h3>Work</h3>
		<ul>
			<li>In section 5.2, please do problems 1, 3, 5, 7, 15, 16, 28, 34.</li>
			<li>In section 5.3, please do problems 1a, 7, 11, 12, 16, 19, 20, 26, 27, 28, 31, 32, 36, 38.</li>
		</ul>

<!-- End of Module 11 -->
<!-- Module 12 - Eigenvalues Begins Here -->
<h2>Week 12 - Eigenvalues</h2>

	<h3>Readings</h3>
		<p>Please read sections 6.1 and 6.2.</p>

	<h3>Words of Advice</h3>
		<ul>
			<li>Eigenvectors are directions where the matrix $A$ acts like scalar multiplication, the scalar it acts like is the eigenvalue.</li>
			<li>I tend to refer to an eigenvector and its eigenvalue as an 'eigenpair'. I intentionally said 'eigenvector and its eigenvalue', because one eigenvalue may have a couple eigenvectors.</li>
			<li>Eigenpairs have seemingly endless uses. In an upcoming section we'll see how they can solve differential equations.</li>
			<li>Eigenpairs also appear in computer graphics, population modeling, dynamics, and a myriad of other applicaitons.</li>
			<li>A Big Picture similar to the front of the book could be created for a two by two matrix with distinct eigenpairs.</li>
			<li>Diagnalizing matricies is a major step in reducing computation time. Computation time of this form is the major cost associated to Machine Learning Algorithms, so reducing it is a major payoff.</li>
			<li>The optional problems in 6.2 (8 and 10) deal with the Fibonacci numbers, You need not do problem 8 to do problem 10. You can do it simply with the recursive definition found in problem 16 on page 267.</li>
			<li>Problem 6.2 26 is a callback to the Big Picture.</li>
			<li>Problem 34 is really really cool. It couples a geometrically obvious fact with a neat proof in the complex numbers. I wonder if there's a proof available along a path of medium difficulty.</li>
			<li>There is a great deal more to eigenvectors to explore including complex values and vectors, generalized eigenvectors, and some serious attempts to get eigenvalues with algebraic or geometric multiplicity greater than one to play nicely. Entire courses exist in these areas.</li>
		</ul>

	<h3>Work</h3>
		<ul>
			<li>In section 6.1, please do problems 1, 2, 5, 6, 9, 10, 12, 14, 21, 25, 32, 37, 38.</li>
			<li>In section 6.2, please do problems 1-4, 6, 7, 8 optional, 10 optional, 11, 14, 15, 26, 34.</li>
		</ul>

<!-- End of Module 12 -->
<!-- Module 13 - Differential Equations and Symmetric Matricies Begins Here -->
<h2>Week 13 - Differential Equations and Symmetric Matricies</h2>

	<h3>Readings</h3>
		<p>Please read sections 6.3 and 6.4.</p>

	<h3>Words of Advice</h3>
		<ul>
			<li>This isn't an entire course in differential equations, but this useful tool is such an easy pickup from where we are that it can't be ignored.</li>
			<li>The definition of the word exponential is 'the rate of change is proportional to the amount'. Written symbolically, this is $y'(t) = ky(t)$.</li>
			<li>Recall (or verify) the critcial fact: if $y'(t) = ky(t)$ then $y(t) = Ae^{kt}$.</li>
			<li>It drives me (and most mathematicians) nuts when people misuse the word 'exponential' to mean 'getting bigger'.</li>
			<li>The concept of an exponential of a matrix is mindblowing, but also actually pretty easy. It can even be calculated without diagnalizing in the special case of a 'nilpotent' matrix. A matrix $A$ is called 'nilpotent' if there exists an integer $n$ such that $A^n =0$. You've encountered matricies like this before. A crowd favorite for this is in problem 11 and another is in 19 don't get fooled by this pattern, there are nilpotent matricies with entirely nonzero entries. Can you find one?</li>
			<li>Symmetry is an incredibly powerful tool, because diagnalization is even easier.</li>
			<li>Symmetric situations arise commonly in applications, so although it seems very niche, it's a pretty common scenario.</li>
			<li>Some of the problems in this section are frustratingly simple, but hard to see.</li>
			<li>Problems 12 and 24 are pretty tricky, don't feel badly if they are hard for you.</li>
		</ul>

	<h3>Work</h3>
		<ul>
			<li>In section 6.3, please do problems 1-4, 8, 11-13, 21, 26, 31a, 32.</li>
			<li>In section 6.4, please do problems 4, 6, 7, 10, 11, 12, 15, 20, 23, 24, 34.</li>
		</ul>

<!-- End of Module 13 -->
<!-- Module 14 - Linear Transformations Begins Here -->
<h2>Week 14 - Linear Transformations</h2>

	<h3>Readings</h3>
		<p>Please read sections 8.1 and 8.2.</p>

	<h3>Words of Advice</h3>
		<ul>
			<li>It may seem obvious that matricies and linear transformations are linked, but that's not a given. Be careful to use only facts from the appropriate area.</li>
			<li>You've been dealing with a great many linear transformations over the semester, so you should be getting pretty good at these kinds of things.</li>
			<li>In section 8.2, we'll see that linear transformations can be represented with a matrix.</li>
			<li>It should be apparent that $T(v) = Av$ is a linear transformation, some thoughts about this relationship are in problem 8.1 11. You might want to start 11 by showing that $T$ is a linear transformation.</li>
			<li>Section 8.2 is the punchline for this course. It shows that Matricies are Linear Transformations and Linear Transformations are Matricies.</li>
			<li>This means that everything we know about Matricies applies to Linear Transformations.</li>
			<li>We now know alot about Matricies, but there are three big takeaways that incorporate almost all of the material. Those are the Big Picture, Determinants, and Eigenpairs.</li>
			<li>In this last section, we also gain a thought about a tool we've been using all along. Diagonalization is really just a change of basis, this is intuitive, but also fantastic.</li>
		</ul>

	<h3>Work</h3>
		<ul>
			<li>In section 8.1, please do problems 1-6, 8, 10, 11, 13, 17, 18, 20ab, 29ac, .</li>
			<li>In section 8.2, please do problems 1-7, 9, 10, 13, 14, 20-22, 26, 27, 34.</li>
		</ul>

<!-- End of Module 14 -->
<!-- Module 15 - Review for Final Exam Begins Here -->
<h2>Week 15 - Review for Final Exam</h2>

	<h3>Words of Advice</h3>
		<ul>
			<li>Fully explain all of your answers to the test. There's no credit for solutions without explanation.</li>
			<li>Explanations should cite facts from the book by page number and location. For example, 'top of page 123' or 'eqn (2) on page 123'.</li>
			<li>Justification should be provided for each step.</li>
			<li>You get credit for sound mathematical reasoning and careful explanation.</li>
			<li>Your explanations should be aimed at your peers level, not at me or Prof. Strang.</li>
			<li>Tests are assumed to be open book and notes, but are individual assessments.</li>
			<li>Please don't collaborate on tests.</li>
			<li>Excellent work making it to the end of the course. I can't imagine it's been an easy journey.</li>
			<li>Thank you for putting in the effort for this class.</li>
			<li>If you'd like something new to learn instead of reviewing, sections 6.5, 8.3, 10.4 and 10.5 all were close contenders for inclusion. 10.5 is probably the coolest of them.</li>
			<li>As with the previous exams, clear explanations are the only thing worth points on the final.</li>
			<li>Good Luck!</li>
		</ul>

<!-- End of Module 15 -->
<!-- End of code generated using Learning Module Creator -->

